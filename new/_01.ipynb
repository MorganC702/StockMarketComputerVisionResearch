{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378effea",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/btc_1min_clean_2018_2025H1.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m; sys.path.append(\u001b[33m'\u001b[39m\u001b[33m../\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df_1m = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data/btc_1min_clean_2018_2025H1.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# df_1m = df_1m.tail(1000000)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data-science/MSDSResearchProjects/.venv/lib/python3.13/site-packages/pandas/io/parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data-science/MSDSResearchProjects/.venv/lib/python3.13/site-packages/pandas/io/parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data-science/MSDSResearchProjects/.venv/lib/python3.13/site-packages/pandas/io/parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data-science/MSDSResearchProjects/.venv/lib/python3.13/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './data/btc_1min_clean_2018_2025H1.parquet'"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append('../')\n",
    "import pandas as pd\n",
    "\n",
    "df_1m = pd.read_parquet(\"./data/btc_1min_clean_2018_2025H1.parquet\")\n",
    "# df_1m = df_1m.tail(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae7165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b355e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename all columns to TitleCase\n",
    "df_1m.columns = [col.capitalize() for col in df_1m.columns]\n",
    "\n",
    "# also rename window_start → Date\n",
    "df_1m = df_1m.rename(columns={\"Window_start\": \"Date\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d2e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_generation import MultiTimeframeImageGen\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- prep dataframe ---\n",
    "df_1m['Date'] = pd.to_datetime(df_1m['Date'])\n",
    "df_1m = df_1m.set_index('Date').sort_index()\n",
    "\n",
    "# --- init ---\n",
    "gen = MultiTimeframeImageGen(\n",
    "    timeframes=[\"1m\", \"3m\", \"5m\", \"15m\", \"1h\", \"4h\", \"1d\"],\n",
    "    base_tf=\"1m\",\n",
    "    output_root=\"./dataset\",\n",
    "    window_sizes={\n",
    "        \"1m\": 60,   \n",
    "        \"3m\": 60,   \n",
    "        \"5m\": 60,   \n",
    "        # \"15m\":60,   \n",
    "        # \"1h\": 60,  \n",
    "        # \"4h\": 60,  \n",
    "        # \"1d\": 60 \n",
    "    }\n",
    ")\n",
    "\n",
    "# --- preload ---\n",
    "minutes_needed = max(\n",
    "    gen.window_sizes[tf] * gen.tf_to_minutes[tf]\n",
    "    for tf in gen.timeframes\n",
    ")\n",
    "preload_df = df_1m.iloc[:minutes_needed]\n",
    "for ts, row in tqdm(preload_df.iterrows(), total=len(preload_df), desc=\"Preloading\"):\n",
    "    row.name = ts\n",
    "    gen.get_last(row, preload=True)\n",
    "\n",
    "# --- generate ---\n",
    "generate_df = df_1m.iloc[minutes_needed:]\n",
    "for ts, row in tqdm(generate_df.iterrows(), total=len(generate_df), desc=\"Generating images\"):\n",
    "    row.name = ts\n",
    "    gen.get_last(row, preload=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def export_tf_video(\n",
    "    root_dir=\"./dataset\",\n",
    "    timeframes_top=(\"1m\", \"3m\", \"5m\", \"15m\"),\n",
    "    timeframes_bottom=(\"1h\", \"4h\", \"1d\"),\n",
    "    output_file=None,\n",
    "    fps=10,\n",
    "    scale=0.7,\n",
    "):\n",
    "    all_timeframes = timeframes_top + timeframes_bottom\n",
    "\n",
    "    if output_file is None:\n",
    "        ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        output_file = f\"{root_dir}/videos/tf_{ts}.mp4\"\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    # Collect files for each timeframe\n",
    "    files = {\n",
    "        tf: sorted(glob.glob(os.path.join(root_dir, tf, \"images\", \"*.png\")))\n",
    "        for tf in all_timeframes\n",
    "    }\n",
    "\n",
    "    # Require all timeframes to have frames\n",
    "    if not all(len(lst) > 0 for lst in files.values()):\n",
    "        raise ValueError(\"⚠️ One or more timeframe folders are empty!\")\n",
    "\n",
    "    num_frames = min(len(lst) for lst in files.values())\n",
    "\n",
    "    # Read one sample to set base sizes\n",
    "    sample = cv2.imread(files[all_timeframes[0]][0])\n",
    "    if sample is None:\n",
    "        raise ValueError(\"⚠️ Could not load sample image.\")\n",
    "\n",
    "    base_height = int(sample.shape[0] * scale)\n",
    "    title_height = 30\n",
    "    chart_height = base_height + title_height\n",
    "\n",
    "    def process_frame(path, label):\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            return None\n",
    "\n",
    "        # Scale consistently to target height\n",
    "        h, w = img.shape[:2]\n",
    "        new_w = int(w * base_height / h)\n",
    "        img = cv2.resize(img, (new_w, base_height))\n",
    "\n",
    "        # Title bar\n",
    "        title_bar = np.full((title_height, img.shape[1], 3), 255, dtype=np.uint8)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        text_size = cv2.getTextSize(label, font, 0.6, 1)[0]\n",
    "        text_x = (title_bar.shape[1] - text_size[0]) // 2\n",
    "        text_y = (title_height + text_size[1]) // 2 - 3  # better centering\n",
    "        cv2.putText(title_bar, label, (text_x, text_y), font, 0.6, (0, 0, 0), 1)\n",
    "\n",
    "        # Stack and border\n",
    "        combined = np.vstack([title_bar, img])\n",
    "        cv2.rectangle(combined, (0, 0), (combined.shape[1] - 1, combined.shape[0] - 1), (0, 0, 0), 1)\n",
    "        return combined\n",
    "\n",
    "    # Determine output canvas size\n",
    "    top_width = sum(process_frame(files[tf][0], tf).shape[1] for tf in timeframes_top)\n",
    "    bot_width = sum(process_frame(files[tf][0], tf).shape[1] for tf in timeframes_bottom)\n",
    "    out_width = max(top_width, bot_width)\n",
    "    out_height = chart_height * 2\n",
    "\n",
    "    # Init video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_file, fourcc, fps, (out_width, out_height))\n",
    "\n",
    "    # Frame loop\n",
    "    for i in range(num_frames):\n",
    "        row_imgs = []\n",
    "        for tf_group in [timeframes_top, timeframes_bottom]:\n",
    "            frames = []\n",
    "            for tf in tf_group:\n",
    "                img = process_frame(files[tf][i], tf)\n",
    "                if img is not None:\n",
    "                    frames.append(img)\n",
    "            if frames:\n",
    "                row = np.hstack(frames)\n",
    "                # Pad to full width if needed\n",
    "                if row.shape[1] < out_width:\n",
    "                    pad = np.full((chart_height, out_width - row.shape[1], 3), 255, dtype=np.uint8)\n",
    "                    row = np.hstack([row, pad])\n",
    "                row_imgs.append(row)\n",
    "\n",
    "        # Stack rows into final frame\n",
    "        full_frame = np.vstack(row_imgs)\n",
    "        out.write(full_frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"✅ Video saved: {output_file}\")\n",
    "\n",
    "\n",
    "# Example run\n",
    "export_tf_video(fps=10, scale=0.6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
