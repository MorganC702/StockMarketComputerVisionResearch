{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e45f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install segmentation-models-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61b13d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import segmentation_models_pytorch as smp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66286a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChartSegDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.images = sorted(os.listdir(image_dir))\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path  = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir,  self.images[idx])\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask  = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Convert mask: 255 = ignore, 100 = resistance, 0 = support\n",
    "        mask_out = np.full_like(mask, fill_value=0)     # background\n",
    "        mask_out[mask == 100] = 1                       # resistance\n",
    "        mask_out[mask == 0] = 2                         # support\n",
    "\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask_out)\n",
    "            image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "        return image, mask.long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c852dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(image_dir, mask_dir, batch_size=8, shuffle=True):\n",
    "    transforms = A.Compose([\n",
    "        A.Resize(640, 640),\n",
    "        A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    ds = ChartSegDataset(image_dir, mask_dir, transforms)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c267e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([4, 3, 640, 640])\n",
      "Mask shape : torch.Size([4, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "# Adjust paths as needed\n",
    "train_loader = get_loader(\n",
    "    image_dir=\"../dataset/images/train\",\n",
    "    mask_dir=\"../dataset/masks/train\",\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "img, msk = batch\n",
    "\n",
    "print(\"Image shape:\", img.shape)  # [B, 3, H, W]\n",
    "print(\"Mask shape :\", msk.shape)  # [B, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2438c458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /Users/morgancooper/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83.3M/83.3M [01:22<00:00, 1.06MB/s]\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name='resnet34',\n",
    "    encoder_weights='imagenet',\n",
    "    in_channels=3,\n",
    "    classes=3  # background, resistance, support\n",
    ").to(device)\n",
    "\n",
    "loss_fn = smp.losses.DiceLoss(mode='multiclass')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0879a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 175/175 [16:47<00:00,  5.76s/it, loss=0.0181] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 done — avg loss: 0.0581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for images, masks in pbar:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        preds = model(images)\n",
    "        loss = loss_fn(preds, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"✅ Epoch {epoch+1} done — avg loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "604ade79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------- 1. Run model on a batch of images ---------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, _ in train_loader:\n",
    "        preds = model(imgs.to(device))  # [B, C, H, W] where C = 2\n",
    "        break  # just one batch for demo\n",
    "\n",
    "# --------- 2. Convert logits to predicted class IDs ---------\n",
    "# Use argmax over channel dim to get class map\n",
    "pred_masks = torch.argmax(preds, dim=1).cpu().numpy()  # [B, H, W]\n",
    "\n",
    "# --------- 3. Pick one mask and convert to color ---------\n",
    "# Define your color map: class 0 = white bg, 1 = red, 2 = green\n",
    "colors = {\n",
    "    0: (255, 255, 255),  # background: white\n",
    "    1: (0, 0, 255),      # resistance: red\n",
    "    2: (0, 255, 0),      # support: green\n",
    "}\n",
    "\n",
    "def mask_to_color(mask):\n",
    "    color_img = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    for cls, color in colors.items():\n",
    "        color_img[mask == cls] = color\n",
    "    return color_img\n",
    "\n",
    "# Pick the first prediction\n",
    "pred_mask_np = pred_masks[0]\n",
    "color_mask = mask_to_color(pred_mask_np)\n",
    "\n",
    "# --------- 4. Save and View using OpenCV (via matplotlib) ---------\n",
    "cv2.imwrite(\"predicted_mask.png\", cv2.cvtColor(color_mask, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# Show in notebook\n",
    "plt.imshow(color_mask)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Predicted Mask\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
