{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92f36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Load data ---\n",
    "df_1m = pd.read_csv(\"./data/agg_data/fx/C:EURUSD_1m_last1y.csv\", parse_dates=[\"timestamp\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5c47c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading 1 days: 100%|██████████| 1440/1440 [00:12<00:00, 117.55min/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preload complete after 1 days. Starting image generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1958/366867 [01:04<3:20:03, 30.40it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     96\u001b[39m lbl_path = save_dir / \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m / tf / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Generate image + labels\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[43mimage_gen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_tf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtail\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandle_limits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m tf_image_paths[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_img\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(img_path)\n\u001b[32m    102\u001b[39m tf_image_paths[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_lbl\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(lbl_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MSDS Research/StockMarketComputerVisionResearch/reinforcement_learning/pipeline/generate_image.py:146\u001b[39m, in \u001b[36mImageGenerator.generate_image\u001b[39m\u001b[34m(self, tf, df, zones, save_path)\u001b[39m\n\u001b[32m    144\u001b[39m candle_limit = \u001b[38;5;28mself\u001b[39m.candle_limits.get(tf, \u001b[32m60\u001b[39m)\n\u001b[32m    145\u001b[39m window = df.tail(candle_limit)\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m \u001b[43mgenerate_zone_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_yolo_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MSDS Research/StockMarketComputerVisionResearch/reinforcement_learning/pipeline/generate_image.py:127\u001b[39m, in \u001b[36mgenerate_zone_image\u001b[39m\u001b[34m(window, visible_zones, save_path, image_size, right_padding_bars, save_yolo_labels)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# --- Save image ---\u001b[39;00m\n\u001b[32m    126\u001b[39m os.makedirs(os.path.dirname(save_path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# --- Save YOLO label ---\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m save_yolo_labels:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from pipeline.aggregator import TimeframeAggregator\n",
    "from pipeline.zone_engine import ZoneEngine\n",
    "from pipeline.generate_image import ImageGenerator\n",
    "\n",
    "# --- Config ---\n",
    "csv_path = \"./data/agg_data/fx/C:EURUSD_1m_last1y.csv\"\n",
    "save_dir = Path(\"./dataset\")\n",
    "timeframes = [\"1m\", \"3m\", \"5m\", \"15m\", \"1h\", \"4h\", \"1d\"]\n",
    "candle_limits = {tf: 60 for tf in timeframes}\n",
    "image_size = (640, 640)\n",
    "preload_days = 1\n",
    "\n",
    "# --- Setup ---\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"timestamp\"])\n",
    "buffer = deque(maxlen=10000)\n",
    "aggregator = TimeframeAggregator(buffer)\n",
    "zone_engine = ZoneEngine(timeframes)\n",
    "image_gen = ImageGenerator(candle_limits, image_size=image_size)\n",
    "\n",
    "# --- Preload period ---\n",
    "preload_minutes = preload_days * 24 * 60\n",
    "pbar = tqdm(total=preload_minutes, desc=f\"Preloading {preload_days} days\", unit=\"min\")\n",
    "\n",
    "for i in range(preload_minutes):\n",
    "    if i >= len(df):\n",
    "        break\n",
    "    bar = df.iloc[i].to_dict()\n",
    "    buffer.append(bar)\n",
    "    resampled = aggregator.resample_all(timeframes)\n",
    "\n",
    "    for tf in timeframes:\n",
    "        df_tf = resampled[tf]\n",
    "        if len(df_tf) < candle_limits[tf]:\n",
    "            continue\n",
    "\n",
    "        last_bar = df_tf.iloc[-1].to_dict()\n",
    "        zone_engine.update(tf, last_bar)  # alignment handled inside ZoneEngine\n",
    "\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "print(f\"[INFO] Preload complete after {preload_days} days. Starting image generation.\")\n",
    "\n",
    "# --- Prepare meta.csv ---\n",
    "meta_path = save_dir / \"meta.csv\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Build header dynamically\n",
    "header = [\"timestamp\", \"close\"]\n",
    "for tf in timeframes:\n",
    "    header.append(f\"{tf}_img\")\n",
    "    header.append(f\"{tf}_lbl\")\n",
    "\n",
    "with open(meta_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "\n",
    "# --- Main loop ---\n",
    "for i in tqdm(range(preload_minutes, len(df))):\n",
    "    bar = df.iloc[i].to_dict()\n",
    "    buffer.append(bar)\n",
    "\n",
    "    resampled = aggregator.resample_all(timeframes)\n",
    "    timestamp = pd.to_datetime(df.iloc[i][\"timestamp\"])\n",
    "    close_price = df.iloc[i][\"close\"]\n",
    "\n",
    "    tf_image_paths = {}\n",
    "\n",
    "    for tf in timeframes:\n",
    "        df_tf = resampled[tf]\n",
    "        if len(df_tf) < candle_limits[tf]:\n",
    "            continue\n",
    "\n",
    "        last_candle = df_tf.iloc[-1]\n",
    "        last_ts = pd.to_datetime(last_candle[\"timestamp\"])\n",
    "\n",
    "        # Align to tf granularity\n",
    "        tf_minutes = zone_engine._parse_tf_to_minutes(tf)\n",
    "        aligned_ts = timestamp.floor(f\"{tf_minutes}min\")\n",
    "\n",
    "        # Only update when both dataset and resampled candle are aligned\n",
    "        if last_ts == aligned_ts and timestamp == aligned_ts:\n",
    "            zone_engine.update(tf, last_candle.to_dict())\n",
    "\n",
    "        # Always fetch zones (may be stale if no new update yet)\n",
    "        zones = zone_engine.get_visible_zones(tf)\n",
    "\n",
    "        # Paths\n",
    "        img_path = save_dir / \"images\" / tf / f\"{timestamp}.png\"\n",
    "        lbl_path = save_dir / \"labels\" / tf / f\"{timestamp}.txt\"\n",
    "\n",
    "        # Generate image + labels\n",
    "        image_gen.generate_image(tf, df_tf.tail(candle_limits[tf]), zones, str(img_path))\n",
    "\n",
    "        tf_image_paths[f\"{tf}_img\"] = str(img_path)\n",
    "        tf_image_paths[f\"{tf}_lbl\"] = str(lbl_path)\n",
    "\n",
    "    # Write one row per 1m bar\n",
    "    if tf_image_paths:\n",
    "        row = {\"timestamp\": timestamp, \"close\": close_price}\n",
    "        row.update(tf_image_paths)\n",
    "\n",
    "        with open(meta_path, \"a\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=header)\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(f\"[DONE] Meta written incrementally to {meta_path}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
