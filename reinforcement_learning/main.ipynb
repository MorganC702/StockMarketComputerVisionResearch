{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea858051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Warming up with first 0 minutes (60 days)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming buffer: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Warmup complete. Starting main generation loop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating images (every 5m, 60-bar windows per TF):   0%|          | 1070/368307 [00:11<1:03:04, 97.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     81\u001b[39m img_path.parent.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     82\u001b[39m lbl_path.parent.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43mimage_gen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m tf_image_paths[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_img\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(img_path.relative_to(save_dir))\n\u001b[32m     87\u001b[39m tf_image_paths[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_lbl\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(lbl_path.relative_to(save_dir))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data-science/MSDSResearchProjects/StockMarketComputerVisionResearch/reinforcement_learning/data_gen/generate_plain_img.py:90\u001b[39m, in \u001b[36mImageGenerator.generate_image\u001b[39m\u001b[34m(self, tf, df, save_path)\u001b[39m\n\u001b[32m     88\u001b[39m candle_limit = \u001b[38;5;28mself\u001b[39m.candle_limits.get(tf, \u001b[32m60\u001b[39m)\n\u001b[32m     89\u001b[39m window = df.tail(candle_limit)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43mgenerate_plain_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_yolo_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data-science/MSDSResearchProjects/StockMarketComputerVisionResearch/reinforcement_learning/data_gen/generate_plain_img.py:50\u001b[39m, in \u001b[36mgenerate_plain_image\u001b[39m\u001b[34m(window, save_path, image_size, save_yolo_labels)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# --- Draw candles ---\u001b[39;00m\n\u001b[32m     49\u001b[39m yolo_labels = []\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_to_x\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclose\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopen\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data-science/MSDSResearchProjects/.venv/lib/python3.13/site-packages/pandas/core/frame.py:1559\u001b[39m, in \u001b[36mDataFrame.iterrows\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1557\u001b[39m using_cow = using_copy_on_write()\n\u001b[32m   1558\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.index, \u001b[38;5;28mself\u001b[39m.values):\n\u001b[32m-> \u001b[39m\u001b[32m1559\u001b[39m     s = \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1560\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mgr.is_single_block:\n\u001b[32m   1561\u001b[39m         s._mgr.add_references(\u001b[38;5;28mself\u001b[39m._mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data-science/MSDSResearchProjects/.venv/lib/python3.13/site-packages/pandas/core/series.py:574\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, data, index, dtype, name, copy, fastpath)\u001b[39m\n\u001b[32m    572\u001b[39m         data = [data]\n\u001b[32m    573\u001b[39m     index = default_index(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_list_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    575\u001b[39m     com.require_length_match(data, index)\n\u001b[32m    577\u001b[39m \u001b[38;5;66;03m# create/copy the manager\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from pipeline.aggregator import TimeframeAggregator\n",
    "from data_gen.generate_plain_img import ImageGenerator  # plain candles + YOLO boxes\n",
    "\n",
    "# --- Config ---\n",
    "csv_path = \"./data/agg_data/fx/C:EURUSD_1m_last1y.csv\"\n",
    "save_dir = Path(\"./dataset\")\n",
    "\n",
    "timeframes = [\"5m\", \"15m\", \"1h\", \"4h\", \"1d\"]\n",
    "candle_limits = {tf: 60 for tf in timeframes}  # always 60 candles per TF\n",
    "image_size = (640, 640)\n",
    "\n",
    "# --- Setup ---\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"timestamp\"])\n",
    "buffer = deque(maxlen=60 * 24 * 60 + 100)\n",
    "aggregator = TimeframeAggregator(buffer)\n",
    "image_gen = ImageGenerator(candle_limits, image_size=image_size)\n",
    "\n",
    "# --- Warmup with first N minutes ---\n",
    "start_index = 0 * 24 * 60  # 60 days * 24h * 60m\n",
    "print(f\"[INFO] Warming up with first {start_index} minutes ({60} days)\")\n",
    "\n",
    "warmup_df = df.iloc[:start_index]\n",
    "for _, row in tqdm(warmup_df.iterrows(), total=len(warmup_df), desc=\"Warming buffer\"):\n",
    "    buffer.append(row.to_dict())\n",
    "    aggregator.resample_all(timeframes)\n",
    "\n",
    "print(\"[INFO] Warmup complete. Starting main generation loop.\")\n",
    "\n",
    "# --- Prepare meta.csv ---\n",
    "meta_path = save_dir / \"meta.csv\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "header = [\"id\", \"timestamp\", \"close\"]\n",
    "for tf in timeframes:\n",
    "    header.append(f\"{tf}_img\")\n",
    "    header.append(f\"{tf}_lbl\")\n",
    "\n",
    "with open(meta_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "\n",
    "# --- Main loop ---\n",
    "file_index = 0  # Counter for generating unique file IDs\n",
    "\n",
    "for i, row in tqdm(df.iloc[start_index:].iterrows(),\n",
    "                   total=len(df) - start_index,\n",
    "                   desc=\"Generating images (every 5m, 60-bar windows per TF)\"):\n",
    "    bar = row.to_dict()\n",
    "    buffer.append(bar)\n",
    "\n",
    "    resampled = aggregator.resample_all(timeframes)\n",
    "    timestamp = pd.to_datetime(row[\"timestamp\"])\n",
    "    close_price = row[\"close\"]\n",
    "\n",
    "    # Only produce outputs on 5m boundaries\n",
    "    if timestamp != timestamp.floor(\"5min\"):\n",
    "        continue\n",
    "\n",
    "    file_index += 1\n",
    "    file_id = f\"{file_index:06d}\"  # e.g. \"000001\"\n",
    "\n",
    "    tf_image_paths = {}\n",
    "    for tf in timeframes:\n",
    "        df_tf = resampled[tf]\n",
    "        if df_tf.empty:\n",
    "            continue\n",
    "\n",
    "        # Always grab up to 60 candles (partial if fewer)\n",
    "        window = df_tf.tail(min(len(df_tf), candle_limits[tf]))\n",
    "\n",
    "        img_path = save_dir / \"images\" / tf / f\"{file_id}.png\"\n",
    "        lbl_path = save_dir / \"labels\" / tf / f\"{file_id}.txt\"\n",
    "\n",
    "        img_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        lbl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        image_gen.generate_image(tf, window, str(img_path))\n",
    "\n",
    "        tf_image_paths[f\"{tf}_img\"] = str(img_path.relative_to(save_dir))\n",
    "        tf_image_paths[f\"{tf}_lbl\"] = str(lbl_path.relative_to(save_dir))\n",
    "\n",
    "    # Write CSV row even if some TFs are missing (they’ll just be blank)\n",
    "    row_out = {\n",
    "        \"id\": file_id,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"close\": close_price,\n",
    "        **tf_image_paths\n",
    "    }\n",
    "\n",
    "    with open(meta_path, \"a\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writerow(row_out)\n",
    "\n",
    "print(f\"[✅ DONE] Meta written with simplified filenames to {meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e921b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67344b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# --- Settings ---\n",
    "SOURCE_IMAGE_DIR = \"./dataset/images\"\n",
    "SOURCE_LABEL_DIR = \"./dataset/labels\"\n",
    "OUTPUT_BASE_DIR = \"./img_dataset\"\n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "VAL_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.1\n",
    "MAX_IMAGES = 200_000\n",
    "\n",
    "# --- Collect all image paths (sorted order) ---\n",
    "image_paths = sorted(\n",
    "    glob.glob(f\"{SOURCE_IMAGE_DIR}/**/*.png\", recursive=True) +\n",
    "    glob.glob(f\"{SOURCE_IMAGE_DIR}/**/*.jpg\", recursive=True)\n",
    ")\n",
    "\n",
    "# --- Limit to MAX_IMAGES ---\n",
    "total = min(len(image_paths), MAX_IMAGES)\n",
    "image_paths = image_paths[:total]\n",
    "\n",
    "# --- Calculate split indices ---\n",
    "n_train = int(total * TRAIN_SPLIT)\n",
    "n_val = int(total * VAL_SPLIT)\n",
    "n_test = total - n_train - n_val\n",
    "\n",
    "train_imgs = image_paths[:n_train]\n",
    "val_imgs = image_paths[n_train:n_train + n_val]\n",
    "test_imgs = image_paths[n_train + n_val:]\n",
    "\n",
    "splits = {\n",
    "    \"train\": train_imgs,\n",
    "    \"val\": val_imgs,\n",
    "    \"test\": test_imgs\n",
    "}\n",
    "\n",
    "# --- Copy files with timeframe in name ---\n",
    "for split, paths in splits.items():\n",
    "    for img_path in paths:\n",
    "        relative_img_path = os.path.relpath(img_path, SOURCE_IMAGE_DIR)\n",
    "        tf = relative_img_path.split(os.sep)[0]  # timeframe (e.g. \"1h\")\n",
    "\n",
    "        filename = os.path.basename(img_path)\n",
    "        new_filename = f\"{tf}_{filename}\"\n",
    "\n",
    "        img_out_dir = os.path.join(OUTPUT_BASE_DIR, \"images\", split)\n",
    "        lbl_out_dir = os.path.join(OUTPUT_BASE_DIR, \"labels\", split)\n",
    "        os.makedirs(img_out_dir, exist_ok=True)\n",
    "        os.makedirs(lbl_out_dir, exist_ok=True)\n",
    "\n",
    "        # Copy image\n",
    "        shutil.copy(img_path, os.path.join(img_out_dir, new_filename))\n",
    "\n",
    "        # Copy label if exists\n",
    "        label_rel_path = os.path.splitext(relative_img_path)[0] + \".txt\"\n",
    "        label_full_path = os.path.join(SOURCE_LABEL_DIR, label_rel_path)\n",
    "\n",
    "        if os.path.exists(label_full_path):\n",
    "            new_label_name = os.path.splitext(new_filename)[0] + \".txt\"\n",
    "            shutil.copy(label_full_path, os.path.join(lbl_out_dir, new_label_name))\n",
    "        else:\n",
    "            print(f\"⚠️ Label not found for: {img_path}\")\n",
    "\n",
    "print(\"✅ Dataset split complete. Filenames now include timeframe.\")\n",
    "print(f\"Total: {total} | Train: {len(train_imgs)} | Val: {len(val_imgs)} | Test: {len(test_imgs)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d8781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "\n",
    "from env.trading_env import TradingEnv\n",
    "from models.yolo_extractor import CustomYOLOPolicy\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# writer = SummaryWriter(log_dir=\"./logs/custom\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Load your meta.csv ---\n",
    "meta_df = pd.read_csv(\"./dataset/meta.csv\", parse_dates=[\"timestamp\"])\n",
    "\n",
    "# --- Create your environment ---\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = TradingEnv(meta_df)\n",
    "        env = Monitor(env)  # Wrap with Monitor to log rewards\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "# Wrap in vectorized and monitored env\n",
    "vec_env = DummyVecEnv([make_env()])\n",
    "vec_env = VecMonitor(vec_env)  # Logs mean reward per episode\n",
    "\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "\n",
    "vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_reward=10.0)\n",
    "\n",
    "# --- Create PPO model ---\n",
    "model = PPO(\n",
    "    policy=CustomYOLOPolicy,\n",
    "    env=vec_env,\n",
    "    verbose=1,\n",
    "    n_steps=128,\n",
    "    batch_size=32,\n",
    "    learning_rate=3e-4,\n",
    "    ent_coef=0.01,\n",
    "    tensorboard_log=\"./logs\"\n",
    ")\n",
    "\n",
    "# After defining your model:\n",
    "# callback = RewardLoggingCallback(writer=writer)\n",
    "\n",
    "\n",
    "# --- Train the model ---\n",
    "\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "vec_env.save(\"vec_normalize.pkl\")\n",
    "\n",
    "# --- Save it ---\n",
    "model.save(\"ppo-yolo-trading\")\n",
    "print(\"✅ Training complete. Model saved to: ppo-yolo-trading\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8b6823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
